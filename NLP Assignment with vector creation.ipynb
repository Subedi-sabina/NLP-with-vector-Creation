{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97835054-a3e7-4ed5-8d6d-b0807684329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffb186d-6c54-4207-9c06-b9cd364d0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and POS Tags:\n",
      "The: DT\n",
      "quick: JJ\n",
      "brown: NN\n",
      "fox: NN\n",
      "jumps: VBZ\n",
      "over: IN\n",
      "the: DT\n",
      "lazy: JJ\n",
      "dog: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Print the tokens with their POS tags\n",
    "print(\"Tokens and POS Tags:\")\n",
    "for token, tag in pos_tags:\n",
    "    print(f\"{token}: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d889e40-6266-45db-bdb5-0fe1d3d1b20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\n",
      "Text After HTML Removal: data science is an interdisciplinary field that uses scientific methods processes algorithms and systems to extract knowledge and insights from structured and unstructured data\n",
      "Tokens: ['data', 'science', 'is', 'an', 'interdisciplinary', 'field', 'that', 'uses', 'scientific', 'methods', 'processes', 'algorithms', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'structured', 'and', 'unstructured', 'data']\n",
      "Filtered Tokens: ['data', 'science', 'interdisciplinary', 'field', 'uses', 'scientific', 'methods', 'processes', 'algorithms', 'systems', 'extract', 'knowledge', 'insights', 'structured', 'unstructured', 'data']\n",
      "Stemmed Tokens: ['data', 'scienc', 'interdisciplinari', 'field', 'use', 'scientif', 'method', 'process', 'algorithm', 'system', 'extract', 'knowledg', 'insight', 'structur', 'unstructur', 'data']\n",
      "Lemmatized Tokens: ['data', 'science', 'interdisciplinary', 'field', 'us', 'scientific', 'method', 'process', 'algorithm', 'system', 'extract', 'knowledge', 'insight', 'structure', 'unstructured', 'data']\n",
      "--------------------------------------------------\n",
      "Original Document: It integrates various domains such as statistics, machine learning, data mining, and big data analytics.\n",
      "Text After HTML Removal: it integrates various domains such as statistics machine learning data mining and big data analytics\n",
      "Tokens: ['it', 'integrates', 'various', 'domains', 'such', 'as', 'statistics', 'machine', 'learning', 'data', 'mining', 'and', 'big', 'data', 'analytics']\n",
      "Filtered Tokens: ['integrates', 'various', 'domains', 'statistics', 'machine', 'learning', 'data', 'mining', 'big', 'data', 'analytics']\n",
      "Stemmed Tokens: ['integr', 'variou', 'domain', 'statist', 'machin', 'learn', 'data', 'mine', 'big', 'data', 'analyt']\n",
      "Lemmatized Tokens: ['integrates', 'various', 'domain', 'statistic', 'machine', 'learn', 'data', 'mining', 'big', 'data', 'analytics']\n",
      "--------------------------------------------------\n",
      "Original Document: Data science is essential for making informed decisions in business, healthcare, finance, and many other industries.\n",
      "Text After HTML Removal: data science is essential for making informed decisions in business healthcare finance and many other industries\n",
      "Tokens: ['data', 'science', 'is', 'essential', 'for', 'making', 'informed', 'decisions', 'in', 'business', 'healthcare', 'finance', 'and', 'many', 'other', 'industries']\n",
      "Filtered Tokens: ['data', 'science', 'essential', 'making', 'informed', 'decisions', 'business', 'healthcare', 'finance', 'many', 'industries']\n",
      "Stemmed Tokens: ['data', 'scienc', 'essenti', 'make', 'inform', 'decis', 'busi', 'healthcar', 'financ', 'mani', 'industri']\n",
      "Lemmatized Tokens: ['data', 'science', 'essential', 'make', 'inform', 'decision', 'business', 'healthcare', 'finance', 'many', 'industry']\n",
      "--------------------------------------------------\n",
      "Original Document: It involves data collection, data cleaning, data analysis, and data visualization.\n",
      "Text After HTML Removal: it involves data collection data cleaning data analysis and data visualization\n",
      "Tokens: ['it', 'involves', 'data', 'collection', 'data', 'cleaning', 'data', 'analysis', 'and', 'data', 'visualization']\n",
      "Filtered Tokens: ['involves', 'data', 'collection', 'data', 'cleaning', 'data', 'analysis', 'data', 'visualization']\n",
      "Stemmed Tokens: ['involv', 'data', 'collect', 'data', 'clean', 'data', 'analysi', 'data', 'visual']\n",
      "Lemmatized Tokens: ['involves', 'data', 'collection', 'data', 'cleaning', 'data', 'analysis', 'data', 'visualization']\n",
      "--------------------------------------------------\n",
      "Original Document: Data scientists use programming languages like Python and R to perform data analysis.\n",
      "Text After HTML Removal: data scientists use programming languages like python and r to perform data analysis\n",
      "Tokens: ['data', 'scientists', 'use', 'programming', 'languages', 'like', 'python', 'and', 'r', 'to', 'perform', 'data', 'analysis']\n",
      "Filtered Tokens: ['data', 'scientists', 'use', 'programming', 'languages', 'like', 'python', 'r', 'perform', 'data', 'analysis']\n",
      "Stemmed Tokens: ['data', 'scientist', 'use', 'program', 'languag', 'like', 'python', 'r', 'perform', 'data', 'analysi']\n",
      "Lemmatized Tokens: ['data', 'scientist', 'use', 'program', 'language', 'like', 'python', 'r', 'perform', 'data', 'analysis']\n",
      "--------------------------------------------------\n",
      "Original Document: They also employ tools such as Hadoop, Spark, and SQL for handling large datasets.\n",
      "Text After HTML Removal: they also employ tools such as hadoop spark and sql for handling large datasets\n",
      "Tokens: ['they', 'also', 'employ', 'tools', 'such', 'as', 'hadoop', 'spark', 'and', 'sql', 'for', 'handling', 'large', 'datasets']\n",
      "Filtered Tokens: ['also', 'employ', 'tools', 'hadoop', 'spark', 'sql', 'handling', 'large', 'datasets']\n",
      "Stemmed Tokens: ['also', 'employ', 'tool', 'hadoop', 'spark', 'sql', 'handl', 'larg', 'dataset']\n",
      "Lemmatized Tokens: ['also', 'employ', 'tool', 'hadoop', 'spark', 'sql', 'handle', 'large', 'datasets']\n",
      "--------------------------------------------------\n",
      "Original Document: The ultimate goal of data science is to uncover hidden patterns, correlations, and trends.\n",
      "Text After HTML Removal: the ultimate goal of data science is to uncover hidden patterns correlations and trends\n",
      "Tokens: ['the', 'ultimate', 'goal', 'of', 'data', 'science', 'is', 'to', 'uncover', 'hidden', 'patterns', 'correlations', 'and', 'trends']\n",
      "Filtered Tokens: ['ultimate', 'goal', 'data', 'science', 'uncover', 'hidden', 'patterns', 'correlations', 'trends']\n",
      "Stemmed Tokens: ['ultim', 'goal', 'data', 'scienc', 'uncov', 'hidden', 'pattern', 'correl', 'trend']\n",
      "Lemmatized Tokens: ['ultimate', 'goal', 'data', 'science', 'uncover', 'hidden', 'pattern', 'correlation', 'trend']\n",
      "--------------------------------------------------\n",
      "Original Document: By doing so, it helps organizations improve their operations and strategies.\n",
      "Text After HTML Removal: by doing so it helps organizations improve their operations and strategies\n",
      "Tokens: ['by', 'doing', 'so', 'it', 'helps', 'organizations', 'improve', 'their', 'operations', 'and', 'strategies']\n",
      "Filtered Tokens: ['helps', 'organizations', 'improve', 'operations', 'strategies']\n",
      "Stemmed Tokens: ['help', 'organ', 'improv', 'oper', 'strategi']\n",
      "Lemmatized Tokens: ['help', 'organization', 'improve', 'operation', 'strategy']\n",
      "--------------------------------------------------\n",
      "Original Document: Data science is a rapidly growing field, with increasing demand for skilled professionals.\n",
      "Text After HTML Removal: data science is a rapidly growing field with increasing demand for skilled professionals\n",
      "Tokens: ['data', 'science', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'increasing', 'demand', 'for', 'skilled', 'professionals']\n",
      "Filtered Tokens: ['data', 'science', 'rapidly', 'growing', 'field', 'increasing', 'demand', 'skilled', 'professionals']\n",
      "Stemmed Tokens: ['data', 'scienc', 'rapidli', 'grow', 'field', 'increas', 'demand', 'skill', 'profession']\n",
      "Lemmatized Tokens: ['data', 'science', 'rapidly', 'grow', 'field', 'increase', 'demand', 'skilled', 'professional']\n",
      "--------------------------------------------------\n",
      "Original Document: It is transforming the way we understand and interact with data, leading to more accurate predictions and better decision-making.\n",
      "Text After HTML Removal: it is transforming the way we understand and interact with data leading to more accurate predictions and better decisionmaking\n",
      "Tokens: ['it', 'is', 'transforming', 'the', 'way', 'we', 'understand', 'and', 'interact', 'with', 'data', 'leading', 'to', 'more', 'accurate', 'predictions', 'and', 'better', 'decisionmaking']\n",
      "Filtered Tokens: ['transforming', 'way', 'understand', 'interact', 'data', 'leading', 'accurate', 'predictions', 'better', 'decisionmaking']\n",
      "Stemmed Tokens: ['transform', 'way', 'understand', 'interact', 'data', 'lead', 'accur', 'predict', 'better', 'decisionmak']\n",
      "Lemmatized Tokens: ['transform', 'way', 'understand', 'interact', 'data', 'lead', 'accurate', 'prediction', 'well', 'decisionmaking']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize the Porter Stemmer and WordNet Lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\",\n",
    "    \"It integrates various domains such as statistics, machine learning, data mining, and big data analytics.\",\n",
    "    \"Data science is essential for making informed decisions in business, healthcare, finance, and many other industries.\",\n",
    "    \"It involves data collection, data cleaning, data analysis, and data visualization.\",\n",
    "    \"Data scientists use programming languages like Python and R to perform data analysis.\",\n",
    "    \"They also employ tools such as Hadoop, Spark, and SQL for handling large datasets.\",\n",
    "    \"The ultimate goal of data science is to uncover hidden patterns, correlations, and trends.\",\n",
    "    \"By doing so, it helps organizations improve their operations and strategies.\",\n",
    "    \"Data science is a rapidly growing field, with increasing demand for skilled professionals.\",\n",
    "    \"It is transforming the way we understand and interact with data, leading to more accurate predictions and better decision-making.\"\n",
    "]\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_tokens]\n",
    "    return {\n",
    "        'original': text,\n",
    "        'tokens': tokens,\n",
    "        'filtered_tokens': filtered_tokens,\n",
    "        'stemmed_tokens': stemmed_tokens,\n",
    "        'lemmatized_tokens': lemmatized_tokens\n",
    "    }\n",
    "\n",
    "# Function to get the part of speech tag for lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Process each document in the corpus\n",
    "for document in corpus:\n",
    "    result = preprocess_text(document)\n",
    "    print(\"Original Document:\", document)\n",
    "    print(\"Text After HTML Removal:\", result['original'])\n",
    "    print(\"Tokens:\", result['tokens'])\n",
    "    print(\"Filtered Tokens:\", result['filtered_tokens'])\n",
    "    print(\"Stemmed Tokens:\", result['stemmed_tokens'])\n",
    "    print(\"Lemmatized Tokens:\", result['lemmatized_tokens'])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1197f58-aa35-47a9-b460-9b0af198b18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Reviews:\n",
      " 0      review 1: this movie is very scary and long.\n",
      "1    review 2: this movie is not scary and is slow.\n",
      "2          review 3: this movie is spooky and good.\n",
      "Name: Review, dtype: object\n",
      "\n",
      "One-Hot Encoded:\n",
      " [[1 0 1 1 1 0 1 1 0 0 1 1]\n",
      " [1 0 1 0 1 1 1 1 1 0 1 0]\n",
      " [1 1 1 0 1 0 1 0 0 1 1 0]]\n",
      "Vocabulary: ['and' 'good' 'is' 'long' 'movie' 'not' 'review' 'scary' 'slow' 'spooky'\n",
      " 'this' 'very']\n",
      "\n",
      "Bag of Words:\n",
      " [[1 0 1 1 1 0 1 1 0 0 1 1]\n",
      " [1 0 2 0 1 1 1 1 1 0 1 0]\n",
      " [1 1 1 0 1 0 1 0 0 1 1 0]]\n",
      "Vocabulary: ['and' 'good' 'is' 'long' 'movie' 'not' 'review' 'scary' 'slow' 'spooky'\n",
      " 'this' 'very']\n",
      "\n",
      "TF-IDF:\n",
      " [[0.28407693 0.         0.28407693 0.48098405 0.28407693 0.\n",
      "  0.28407693 0.36580076 0.         0.         0.28407693 0.48098405]\n",
      " [0.25489296 0.         0.50978591 0.         0.25489296 0.43157129\n",
      "  0.25489296 0.32822109 0.43157129 0.         0.25489296 0.        ]\n",
      " [0.30523155 0.51680194 0.30523155 0.         0.30523155 0.\n",
      "  0.30523155 0.         0.         0.51680194 0.30523155 0.        ]]\n",
      "Vocabulary: ['and' 'good' 'is' 'long' 'movie' 'not' 'review' 'scary' 'slow' 'spooky'\n",
      " 'this' 'very']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample reviews\n",
    "data = {\n",
    "   \"Review\": [\n",
    "        \"Review 1: This movie is very scary and long.\",\n",
    "        \"Review 2: This movie is not scary and is slow.\",\n",
    "        \"Review 3: This movie is spooky and good.\"\n",
    "        ]}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert to lowercase\n",
    "df['Review'] = df['Review'].str.lower()\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = CountVectorizer(binary=True)\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(df['Review'])\n",
    "\n",
    "# Bag of Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_encoded = count_vectorizer.fit_transform(df['Review'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded = tfidf_vectorizer.fit_transform(df['Review'])\n",
    "\n",
    "# Output Results\n",
    "print(\"Original Reviews:\\n\", df['Review'])\n",
    "print(\"\\nOne-Hot Encoded:\\n\", one_hot_encoded.toarray())\n",
    "print(\"Vocabulary:\", one_hot_encoder.get_feature_names_out())\n",
    "\n",
    "print(\"\\nBag of Words:\\n\", bow_encoded.toarray())\n",
    "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF:\\n\", tfidf_encoded.toarray())\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888f484-f60f-4b6c-8e15-4da16e11a503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
